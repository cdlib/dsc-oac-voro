## Rakefile to run the voro flow
# Brian Tingle 2008/March, April / 2010 summer,fall
# This Rakefile runs the weekly voro processing and XTF indexing
# 

#
# 00 18 * * 5 /voro/code/wrappers/bin/cronic /voro/local/bin/rake -f /voro/code/rake/Rakefile
# http://habilis.net/cronic/ was modified to log in /voro/var/log/
# add some locking?

# default task -- do it all from cron
# why not use dependencies?  
# I want to be able to re-run the sub-parts independently in the case of a botched build

require "socket"

branch = 'production'
ENV['PERL5LIB'] = "#{ENV['PREFIX']}/perllib"
ENV['CODEBASE'] = "#{ENV['HOME']}/branches/#{branch}"


task :default do |t|
	if not File.exists?("#{ENV['HOME']}"+"/log/skip-sync") then
		Rake::Task["sync:pullfromproduction"].execute
		Rake::Task["sync:syncdbwithprod"].execute 
	end
	Rake::Task["ead:makemets"].invoke
	Rake::Task[:DCme].invoke
	Rake::Task["ead:pdfgen"].invoke
	Rake::Task["ead:pdfsweep"].invoke
	Rake::Task["ead:oisindex"].invoke
	Rake::Task["xtf:metsark"].invoke
        # Rake::Task["tar-index"].invoke
	# Rake::Task["s3-push"].invoke
 	Rake::Task["ingest-stats"].invoke
end

# re-use the TWS wrappers and dependencies
# (TWS wrappers log things in /voro/var/log)
namespace :ead do
	desc "build EAD files"
	
	desc "make METS from the EAD"
	task :makemets do |t|
                ENV['VOROBASE'] = "#{ENV['HOME']}/branches/#{branch}/voro"
                puts "Current env is #{ENV['PERL5LIB']}"
                sh "perl #{ENV['CODEBASE']}/voro/batch-bin/reMets.pl"
                # sh "perl #{ENV['CODEBASE']}/voro/batch-bin/removeMets.pl"
		# Regenerate all METS [VORO update #6]
		# perl ${VOROBASE}/batch-bin/reMets.pl
		# reads workspace/dlxs/prime2002 
		# and outputs METS to /voro/data/oac-ead/data 
		# Regenerate METS for removed items (200502)
		# perl ${VOROBASE}/batch-bin/removeMets.pl
		# reads workspace/dlxs/remove and 
		# outputs METS to /voro/data/oac-ead/data
		# WRONG: METS are going to workspace/dlxs/oac-ead/mets
		# FIXED; need to see if this fixes removal issue
	end

	desc "sweep pdf files to dynaXML directory structure"
	task :pdfsweep do |t|
		# this should be "temporary", I didn't have time
		# to clean this up, I'd like for PDF to have
		# the same filename as the EAD
		sh "#{ENV['HOME']}/branches/#{branch}/voro/batch-bin/pdfDlxs2dyxaXML.pl"
	end

	desc "build sqlite3 database for ois wsgi service"
	task :oisindex do |t|
		# scans files in /voro/XTF/data and uses the django database to build
		# an sqlite database /voro/code/oac4/ois/ois.sqlite3
		# used later in prefilter via /wsgi/ois_service.wsgi?ark=&parent_ark=
		sh "#{ENV['HOME']}/branches/#{branch}/voro/objinfo/oisIndexer.py"
                # sh "#{ENV['PREFIX']}/bin/python 
		# /voro/code/oac4/ois/oisIndexer.py  >&! /voro/code/oac4/ois/oisIndexer.log
	end

	desc "get shadow files for PDFs of EADs"
	task :pdfgen do |t|
		sh "sh /dsc/data/pdfshadow/REFRESH.sh"
	end

end

# This replaces rsync.sh and RSYNCDATA
namespace :sync do
	# temporary / phase 2
	desc "pull files from production TEMPORARY"
	task :pullfromproduction do |t|
		where_am_i = Socket.gethostname
		case where_am_i
		when "dscl-dev", "dsc-dsc-dev"
			# need to allow preview files on -dev
			cvs_opts = "-v --update --cvs-exclude --links --recursive --times"
			sh "rsync #{cvs_opts} dsc@dsc-dsc-prd.cdlib.org:/dsc/data/xtf/ #{ENV['HOME']}/data/xtf"
			sh "rsync #{cvs_opts} dsc@dsc-dsc-prd.cdlib.org:/dsc/data/in/ #{ENV['HOME']}/data/in"
		when "dsc-dsc-stg"
			# stage should be a clone of production
			cvs_opts = "-v --delete --cvs-exclude --links --recursive --times"
			sh "rsync #{cvs_opts} dsc@dsc-dsc-prd.cdlib.org:/dsc/data/xtf/ #{ENV['HOME']}/data/xtf"
			sh "rsync #{cvs_opts} dsc@dsc-dsc-prd.cdlib.org:/dsc/data/in/ #{ENV['HOME']}/data/in"
		else
			# cvs_opts = "-v --delete --cvs-exclude --links --recursive --times"
			# hey; I *am* the master for this, I can't sync from myself, can I?
		end
		# no /dsc is yet the master for EAD yet; all must sync from voro
		# production = "voro@voro.cdlib.org"
		# sh "rsync #{cvs_opts} #{production}:/voro/data/oac-ead/prime2002/ #{ENV['HOME']}/data/in/oac-ead/prime2002"
		# sh "rsync #{cvs_opts} #{production}:/voro/data/oac-ead/submission/ #{ENV['HOME']}/data/in/oac-ead/submission"
		# sh "rsync #{cvs_opts} #{production}:/voro/data/oac-ead/remove/ #{ENV['HOME']}/data/in/oac-ead/remove"
		# sh "rsync #{cvs_opts} #{production}:/voro/XTF/data-nonark/marc/ #{ENV['HOME']}/data/xtf/data/marc"
        end

    desc "sync database from production"
    task :syncdbwithprod do |t|
		where_am_i = Socket.gethostname
		case where_am_i
        when "dscl-dev" #production machine syncs
            sh "#{ENV['HOME']}/bin/sync-django-db.py -to-db-key=dev"
        when "dscl-stg"
            sh "#{ENV['HOME']}/bin/sync-django-db.py -to-db-key=stage"
		end
    end

end

# This replaces the DCme TWS shell wrapper
desc "Create Dublin Core records"
task :DCme do |t|
        # /dsc/branches/production/voro/batch-bin/DCme
	sh "#{ENV['HOME']}/branches/#{branch}/voro/batch-bin/DCme"
	# 1. create the build-dc.xml ant build file using a perl script
	#  /voro/mets-support, /voro/workspace/DC/build-dc.xml 
	#  and /voro/XTF/data  are  hardcoded in DCme perl code

	sh "ant -f #{ENV['HOME']}/workspace/DC/build-dc.xml"
	# 2. run ant; needs ant >= 1.6.0
	# works on files in /voro/XTF/data; puts .dc.xml in the same tree
end

# these tasks do the XTF indexing and replace the shell wrappers
# that were in /xtf/bin/index* (but are for the /voro/XTF/ version of the index
namespace :xtf do
	# common XTF task variables
	xtf_home = "#{ENV['HOME']}/branches/#{branch}/xtf"
	textIndexer = "#{xtf_home}/bin/textIndexer"

	desc "Index METS, TEI, EAD with ARKs"
	task :metsark do |t|
		sh "#{textIndexer} -nooptimize -rotate -index cdl"
	end

end

desc "run the ingest stats reports"
task "ingest-stats" do |t|
	sh "#{ENV['HOME']}/branches/#{branch}/voro/batch-bin/voro_ingest_stats.pl"
end

desc "tar up the index for backup and delivery"
task "s3-push" do |t|
    if ['leonardo', 'davinci', 'dscl-stg'].include? Socket.gethostname
        sh "/dsc/local/bin/aws s3 cp /dsc/indexes/xtf.backup.tar s3://xtf.dsc.cdlib.org/indexes/xtf.backup.tar"
        sh "/dsc/local/bin/aws s3 sync --delete /dsc/data/xtf/data/ s3://xtf.dsc.cdlib.org/data/"
        sh "/dsc/local/bin/aws s3 sync --delete /dsc/data/in/ s3://xtf.dsc.cdlib.org/input/"
    end
end 

desc "tar up the index for backup and delivery"
task "tar-index" do |t|
    sleep 60
    sh "/bin/tar cf /dsc/indexes/xtf.backup.tar -C /dsc/branches/production/xtf index"
end
